{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1784b2-cd95-449d-8016-697fb054056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calibrate_camera_from_folder(folder_path, chessboard_size, square_size):\n",
    "    ''' Calibrate camera using images of a chessboard pattern taken from a folder.\n",
    "    Args: \n",
    "        folder_path: Path to the folder containing images of a chessboard pattern\n",
    "        chessboard_size: Tuple of INNER corners in the chessboard pattern, e.g., (11, 7) for a 12x8 chessboard pattern\n",
    "        square_size: Size of a square in the chessboard pattern in millimeters\n",
    "    '''\n",
    "    objp = np.zeros((chessboard_size[0] * chessboard_size[1], 3), np.float32)\n",
    "    # generate coordinates of chessboard corners with mgrid\n",
    "    objp[:, :2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)\n",
    "    # multiply by square_size to get the actual real-world metric coordinates\n",
    "    objp *= square_size\n",
    "\n",
    "    objpoints = []  # 3D points in real-world space, e.g., (0,0,0), (1,0,0), (2,0,0) ....\n",
    "    imgpoints = []  # 2D points in image plane - inner corners of the chessboard\n",
    "\n",
    "    images = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    for image_path in images:\n",
    "        img = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, chessboard_size, None)\n",
    "\n",
    "        if ret:\n",
    "            objpoints.append(objp) # Add the 3D real-world coordinates\n",
    "            imgpoints.append(corners) # Add the corresponding 2D image coordinates\n",
    "\n",
    "            # Draw and display the corners\n",
    "            cv2.drawChessboardCorners(img, chessboard_size, corners, ret)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')  # Turn off axes for cleaner display\n",
    "            plt.show()\n",
    "\n",
    "    # Calibrate the camera - Zhang's method\n",
    "    ret, camera_matrix, dist_coeffs, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "    if ret:\n",
    "        print(\"Camera calibration successful!\")\n",
    "        print(\"Camera matrix:\")\n",
    "        print(camera_matrix)\n",
    "        print(\"Distortion coefficients:\")\n",
    "        print(dist_coeffs)\n",
    "    else:\n",
    "        print(\"Camera calibration failed.\")\n",
    "\n",
    "    return ret, camera_matrix, dist_coeffs, rvecs, tvecs\n",
    "\n",
    "folder_path = \"chessboard\"\n",
    "chessboard_size = (11, 7)  # Adjust based on your chessboard pattern (inner corners)\n",
    "square_size = 40.0  # Size of a square in millimeters\n",
    "\n",
    "# Run calibration\n",
    "ret, camera_matrix, dist_coeffs, rvecs, tvecs = calibrate_camera_from_folder(folder_path, chessboard_size, square_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02451238-e701-4d0f-83a6-3c18ef508de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load two images for initial registration\n",
    "folder_path = \"fountain_dataset\"\n",
    "# Load all images in the folder and sort them\n",
    "image_paths = sorted(\n",
    "    [os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n",
    ")\n",
    "image1 = cv2.imread(image_paths[0]) \n",
    "image2 = cv2.imread(image_paths[1])\n",
    "\n",
    "# Check if images are loaded successfully\n",
    "if image1 is None:\n",
    "    print(\"Error: Could not load image\")\n",
    "    exit()\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Turn off axes for cleaner display\n",
    "plt.show()\n",
    "\n",
    "if image2 is None:\n",
    "    print(\"Error: Could not load image\")\n",
    "    exit()\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Turn off axes for cleaner display\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec07e1a-59a6-4650-aa2e-b801bf0521c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_draw_features(image, detector_type='SIFT'):\n",
    "    ''' Detect features in an image using a feature detector.\n",
    "    Args:\n",
    "        image: The input image\n",
    "        detector_type: Type of feature detector to use. Default is 'SIFT'. Other options are 'ORB' and 'AKAZE'.\n",
    "    '''\n",
    "    if image is None:\n",
    "        print(f\"Error: baaad inmage\")\n",
    "        return\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize the feature detector\n",
    "    if detector_type == 'SIFT':\n",
    "        detector = cv2.SIFT_create()\n",
    "    elif detector_type == 'ORB':\n",
    "        detector = cv2.ORB_create()\n",
    "    elif detector_type == 'AKAZE':\n",
    "        detector = cv2.AKAZE_create()\n",
    "    else:\n",
    "        print(f\"Error: Unsupported detector type '{detector_type}'. Use 'SIFT', 'ORB', or 'AKAZE'.\")\n",
    "        return\n",
    "\n",
    "    # Detect features and compute descriptors\n",
    "    keypoints, descriptors = detector.detectAndCompute(gray, None)\n",
    "\n",
    "    # Draw the keypoints on the image\n",
    "    output_image = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    # Convert BGR to RGB for displaying with matplotlib\n",
    "    output_image_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the result\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(output_image_rgb)\n",
    "    plt.title(f\"Feature Detection with {detector_type}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Detected {len(keypoints)} features using {detector_type}.\")\n",
    "\n",
    "    return keypoints, descriptors\n",
    "    \n",
    "# Detect features and descriptors in the two images, make sure to use the same detector for both images\n",
    "keys1, descs1 = detect_and_draw_features(image1, detector_type='SIFT')\n",
    "keys2, descs2 = detect_and_draw_features(image2, detector_type='SIFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd29144-0966-469d-a011-18b6d34aecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_and_draw_features(image1, image2, keypoints1, descriptors1, keypoints2, descriptors2, matcher_type='BF', metric='L2'):\n",
    "    ''' Match features between two images using a feature matcher.\n",
    "    Args:\n",
    "        image1: The first input image\n",
    "        image2: The second input image\n",
    "        keypoints1: Keypoints detected in the first image\n",
    "        descriptors1: Descriptors computed for the keypoints in the first image\n",
    "        keypoints2: Keypoints detected in the second image\n",
    "        descriptors2: Descriptors computed for the keypoints in the second image\n",
    "        matcher_type: Type of feature matcher to use. Default is 'BF'. Other option is 'FLANN'.\n",
    "        metric: Distance metric to use for matching. Default is 'L2'. Other option is 'L1' for Manhattan distance\n",
    "    '''\n",
    "    if matcher_type == 'BF':\n",
    "        # Brute-force matcher\n",
    "        matcher = cv2.BFMatcher(metric if metric == 'L1' else cv2.NORM_L2, crossCheck=True)\n",
    "    elif matcher_type == 'FLANN':\n",
    "        # FLANN-based matcher\n",
    "        index_params = dict(algorithm=1, trees=5)  # Algorithm 1 = KDTree\n",
    "        search_params = dict(checks=50)  # Number of checks\n",
    "        matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    else:\n",
    "        print(f\"Error: Unsupported matcher type '{matcher_type}'. Use 'BF' or 'FLANN'.\")\n",
    "        return\n",
    "\n",
    "    # Match descriptors\n",
    "    matches = matcher.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Draw matches\n",
    "    matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[::5], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # Convert BGR to RGB for displaying with matplotlib\n",
    "    matched_image_rgb = cv2.cvtColor(matched_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the matched image\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(matched_image_rgb)\n",
    "    plt.title(f\"Feature Matches ({matcher_type})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Number of matches: {len(matches)}\")\n",
    "\n",
    "    return matches\n",
    "\n",
    "# match features in the two images\n",
    "matches = match_and_draw_features(image1, image2, keys1, descs1, keys2, descs2, matcher_type='BF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5ef96-fcaa-463d-adb9-4c0ca915f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projection_matrices_and_filter_matches(matches, keypoints1, keypoints2, K):\n",
    "    ''' Compute the projection matrices of the two cameras and filter matches using the essential matrix.\n",
    "    Args:\n",
    "        matches: List of matches between the two images\n",
    "        keypoints1: Keypoints detected in the first image\n",
    "        keypoints2: Keypoints detected in the second image\n",
    "        K: Camera intrinsics matrix\n",
    "    '''\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.array([keypoints1[m.queryIdx].pt for m in matches], dtype=np.float32)\n",
    "    pts2 = np.array([keypoints2[m.trainIdx].pt for m in matches], dtype=np.float32)\n",
    "\n",
    "    # we have 2 options\n",
    "    # either 1. compute Fundamental matrix and using K, compute Essential matrix\n",
    "    # or 2. directly compute Essential matrix\n",
    "    \n",
    "    # option 1\n",
    "    #F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC, 1.0, 0.999)\n",
    "    #E = K.T @ F @ K\n",
    "\n",
    "    # option 2\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "\n",
    "    # Filter matches using the mask\n",
    "    inlier_matches = [m for m, inlier in zip(matches, mask.ravel()) if inlier]\n",
    "    pts1_inliers = pts1[mask.ravel() == 1]\n",
    "    pts2_inliers = pts2[mask.ravel() == 1]\n",
    "\n",
    "    # Recover pose (R and T) from the essential matrix\n",
    "    _, R, T, mask_pose = cv2.recoverPose(E, pts1_inliers, pts2_inliers, K)\n",
    "\n",
    "    # Form the projection matrices\n",
    "    P1 = np.dot(K, np.hstack((np.eye(3), np.zeros((3, 1)))))  # First camera at origin of world coordinates\n",
    "    P2 = np.dot(K, np.hstack((R, T)))  # Second camera with R, T\n",
    "\n",
    "    print(\"Essential Matrix (E):\")\n",
    "    print(E)\n",
    "    print(\"\\nRotation (R):\")\n",
    "    print(R)\n",
    "    print(\"\\nTranslation (T):\")\n",
    "    print(T)\n",
    "    print(f\"\\nNumber of inlier matches: {len(inlier_matches)}\")\n",
    "    print(\"\\nProjection Matrix of Camera 1 (P1):\")\n",
    "    print(P1)\n",
    "    print(\"\\nProjection Matrix of Camera 2 (P2):\")\n",
    "    print(P2)\n",
    "\n",
    "    return P1, P2, R, T, inlier_matches, pts1_inliers, pts2_inliers, mask.ravel()\n",
    "\n",
    "# Visualize the inlier matches (optional)\n",
    "def visualize_inlier_matches(image1, image2, keypoints1, keypoints2, inlier_matches):\n",
    "    matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, inlier_matches[::10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    matched_image_rgb = cv2.cvtColor(matched_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(matched_image_rgb)\n",
    "    plt.title(f\"Inlier Matches ({len(inlier_matches)} matches)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Intrinsics matrix (K) for fountain dataset\n",
    "# It is different camera from the one used in the chessboard calibration\n",
    "# todo: make consistent\n",
    "K = np.array([\n",
    "    [689.870116, -0.001629, 380.172501],  # fx, 0, cx\n",
    "    [0, 691.039531, 251.702243],  # 0, fy, cy\n",
    "    [0, 0, 1]       # 0, 0, 1\n",
    "], dtype=np.float64)\n",
    "\n",
    "# Compute projection matrices and filter matches\n",
    "P1, P2, R, T, inlier_matches, pts1_inliers, pts2_inliers, mask = compute_projection_matrices_and_filter_matches(matches, keys1, keys2, K)\n",
    "\n",
    "# camera poses - we will keep adding the camera poses as we move along\n",
    "camera_poses = [np.eye(4)]  # first camera is at the origin\n",
    "camera_poses.append(np.hstack((R, T))) # second camera pose\n",
    "\n",
    "# Visualize inlier matches\n",
    "visualize_inlier_matches(image1, image2, keys1, keys2, inlier_matches);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff26702-2e49-43b9-b989-50e74bafdce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def triangulate_3d_points(P1, P2, pts1, pts2):\n",
    "    ''' Triangulate 3D points from corresponding 2D points in two images.\n",
    "    Args:\n",
    "        P1, P2: Projection matrices of camera 1 and camera 2\n",
    "        pts1, pts2: Corresponding 2D points in the two images\n",
    "    '''\n",
    "\n",
    "    # Ensure points are in the correct shape (2xN for triangulation)\n",
    "    pts1_homogeneous = pts1.T\n",
    "    pts2_homogeneous = pts2.T\n",
    "\n",
    "    # Perform triangulation (returns 4D homogeneous coordinates)\n",
    "    points_4d_homogeneous = cv2.triangulatePoints(P1, P2, pts1_homogeneous, pts2_homogeneous)\n",
    "\n",
    "    # Convert homogeneous coordinates to 3D (divide by w)\n",
    "    points_3d = points_4d_homogeneous[:3] / points_4d_homogeneous[3]\n",
    "\n",
    "    # Transpose to return points in Nx3 format\n",
    "    return points_3d.T\n",
    "\n",
    "def update_structure_and_map(structure, keypoint_to_structure_map, keys1, keys2, inlier_matches, image1_id, image2_id, points_3d):\n",
    "    ''' Update the 3D structure and keypoint-to-structure map using the new 3D points.\n",
    "    Args:\n",
    "        structure: The current 3D structure\n",
    "        keypoint_to_structure_map: Mapping of keypoints to 3D points\n",
    "        keys1, keys2: Keypoints detected in the two images\n",
    "        inlier_matches: List of inlier matches between the two images\n",
    "        image1_id, image2_id: IDs of the two images\n",
    "        points_3d: new triangulated 3D points\n",
    "    '''\n",
    "    # Add 3D points to the structure and update the map\n",
    "    for i, match in enumerate(inlier_matches):\n",
    "        # key will be touple [image_id, keypoint_id]\n",
    "        kp_id1 = (image1_id, match.queryIdx)\n",
    "        kp_id2 = (image2_id, match.trainIdx)\n",
    "\n",
    "        # Add the 3D point to the structure\n",
    "        point_idx = len(structure)  # Index of the new point\n",
    "        structure.append(points_3d[i])  # Add point to structure\n",
    "\n",
    "        # Map keypoints to the new 3D point index\n",
    "        keypoint_to_structure_map[kp_id1] = point_idx\n",
    "        keypoint_to_structure_map[kp_id2] = point_idx\n",
    "\n",
    "def plot_3d_points_interactive(points_3d, camera_poses, x_range=(-5, 5), y_range=(-5, 5), z_range=(0, 10)):\n",
    "    ''' Plot 3D points and camera centers interactively using Plotly.\n",
    "    Args:\n",
    "        points_3d: 3D points to plot (Nx3 array).\n",
    "        camera_poses: List of [R|t] matrices (3x4 arrays) for camera extrinsics.\n",
    "        x_range, y_range, z_range: Ranges for the X, Y, Z axes.\n",
    "    '''\n",
    "    # Crop points to the specified ranges\n",
    "    mask = (\n",
    "        (points_3d[:, 0] >= x_range[0]) & (points_3d[:, 0] <= x_range[1]) &  # x range\n",
    "        (points_3d[:, 1] >= y_range[0]) & (points_3d[:, 1] <= y_range[1]) &  # y range\n",
    "        (points_3d[:, 2] >= z_range[0]) & (points_3d[:, 2] <= z_range[1])    # z range\n",
    "    )\n",
    "    cropped_points = points_3d[mask]\n",
    "\n",
    "    # Extract camera centers from [R|t] matrices\n",
    "    camera_centers = []\n",
    "    for pose in camera_poses:\n",
    "        R = pose[:, :3]  # Rotation matrix (3x3)\n",
    "        t = pose[:, 3]   # Translation vector (3x1)\n",
    "        C = -np.dot(R.T, t)  # Camera center in world coordinates\n",
    "        camera_centers.append(C)\n",
    "    camera_centers = np.array(camera_centers)  # Convert to Nx3 array\n",
    "\n",
    "    # Create a 3D scatter plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add 3D points\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=cropped_points[:, 0],  # X-coordinates\n",
    "        y=cropped_points[:, 2],  # Y-coordinates\n",
    "        z=-cropped_points[:, 1],  # Z-coordinates\n",
    "        mode='markers',\n",
    "        marker=dict(size=3, color=cropped_points[:, 2], colorscale='Viridis', opacity=0.8),\n",
    "        name=\"3D Points\"\n",
    "    ))\n",
    "\n",
    "    # Add camera centers\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=camera_centers[:, 0],  # X-coordinates of cameras\n",
    "        y=camera_centers[:, 2],  # Y-coordinates of cameras\n",
    "        z=-camera_centers[:, 1],  # Z-coordinates of cameras\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color='red', symbol='x'),\n",
    "        name=\"Camera Centers\"\n",
    "    ))\n",
    "\n",
    "    # Set plot layout\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        title=\"Interactive 3D Point Cloud with Camera Centers\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Perform triangulation\n",
    "points_3d = triangulate_3d_points(P1, P2, pts1_inliers, pts2_inliers)\n",
    "\n",
    "# we will keep track of the 3D points and the mapping of keypoints to 2D points\n",
    "# structure will store the 3D points ->\n",
    "structure = []\n",
    "# keypoint_to_structure_map will store the mapping of keypoints to 3D points\n",
    "keypoint_to_structure_map = {}\n",
    "\n",
    "# store the 3D points and update the map\n",
    "update_structure_and_map(structure, keypoint_to_structure_map, keys1, keys2, inlier_matches, 0, 1, points_3d)\n",
    "\n",
    "# Print the 3D points\n",
    "print(\"Triangulated {points_3d.shape[0]} 3D points:\")\n",
    "\n",
    "# Plot the triangulated points interactively\n",
    "plot_3d_points_interactive(points_3d, camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a16418-3e0c-4ffe-ada8-4f5e5840a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will add a new image to the structure\n",
    "\n",
    "def find_2d_3d_correspondences(matches, keypoints_new, image_id_past):\n",
    "    ''' Find 2D-3D correspondences between two images using the matches and the keypoint-to-structure map.\n",
    "    Args:\n",
    "        matches: List of matches between the two images\n",
    "        keypoints1, keypoints2: Keypoints detected in the two images\n",
    "        image1_id, image2_id: IDs of the two images\n",
    "    '''\n",
    "    pts_2d = []\n",
    "    pts_3d = []\n",
    "\n",
    "    for match in matches:\n",
    "        # look for the keypoint in the previous image\n",
    "        kp_id = (image_id_past, match.queryIdx)\n",
    "\n",
    "        # Check if the keypoint in the previous image is mapped to a 3D point\n",
    "        if kp_id in keypoint_to_structure_map:\n",
    "            # Retrieve the 3D point index\n",
    "            point_3d_idx = keypoint_to_structure_map[kp_id]\n",
    "\n",
    "            # Get the 2D point from the current image (trainIdx)\n",
    "            pts_2d.append(keypoints_new[match.trainIdx].pt)\n",
    "\n",
    "            # Retrieve the corresponding 3D point from the structure\n",
    "            pts_3d.append(structure[point_3d_idx])\n",
    "\n",
    "    return np.array(pts_2d, dtype=np.float32), np.array(pts_3d, dtype=np.float32)\n",
    "\n",
    "def compute_pose_p3p(K, pts_3d, pts_2d):\n",
    "    ''' Compute the camera pose using the Perspective-n-Point (P3P) algorithm.\n",
    "    Args:\n",
    "        K: Camera intrinsics matrix\n",
    "        pts_3d: 3D points in world coordinates\n",
    "        pts_2d: 2D points in image coordinates\n",
    "    '''\n",
    "    success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "        pts_3d, pts_2d, K, None, flags=cv2.SOLVEPNP_P3P\n",
    "    )\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"P3P failed to compute a valid pose.\")\n",
    "\n",
    "    # Convert rotation vector to a rotation matrix (opencv represents rotation as a vector)\n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "\n",
    "    return R, tvec, inliers\n",
    "\n",
    "def filter_matches_using_epipolar_constraint(matches, keypoints1, keypoints2, threshold=0.1):\n",
    "    ''' Filter matches using the epipolar constraint.\n",
    "    Args:\n",
    "        matches: List of matches between the two images\n",
    "        keypoints1, keypoints2: Keypoints detected in the two images\n",
    "        threshold: Threshold for the epipolar error\n",
    "    '''\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.array([keypoints1[m.queryIdx].pt for m in matches], dtype=np.float32)\n",
    "    pts2 = np.array([keypoints2[m.trainIdx].pt for m in matches], dtype=np.float32)\n",
    "\n",
    "    # Convert keypoints to homogeneous coordinates\n",
    "    pts1_homogeneous = np.hstack((pts1, np.ones((pts1.shape[0], 1))))\n",
    "    pts2_homogeneous = np.hstack((pts2, np.ones((pts2.shape[0], 1))))\n",
    "\n",
    "    # Compute the fundamental matrix\n",
    "    F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC, threshold, 0.999)\n",
    "\n",
    "    # Filter matches using the mask\n",
    "    inlier_matches = [m for m, inlier in zip(matches, mask.ravel()) if inlier]\n",
    "\n",
    "    print(f\"Number of matches before filtering: {len(matches)}\")\n",
    "    print(f\"Number of matches after filtering: {len(inlier_matches)}\")\n",
    "\n",
    "    return inlier_matches\n",
    "\n",
    "def compute_projection_matrix(K, R, T):\n",
    "    ''' Compute the projection matrix from the camera pose.\n",
    "    Args:\n",
    "        K: Camera intrinsics matrix\n",
    "        R: Rotation matrix\n",
    "        T: Translation vector\n",
    "    '''\n",
    "    P = np.dot(K, np.hstack((R, T)))\n",
    "    return P\n",
    "\n",
    "def triangulate_3d_points_matches(P1, P2, kpts1, kpts2, matches):\n",
    "    ''' Triangulate 3D points from matched keypoints in two images.\n",
    "    Args:\n",
    "        P1, P2: Projection matrices of camera 1 and camera 2\n",
    "        kpts1, kpts2: Keypoints detected in the two images\n",
    "        matches: List of matches between the two images\n",
    "    '''\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.array([kpts1[m.queryIdx].pt for m in matches], dtype=np.float32).T  # Shape: 2xN\n",
    "    pts2 = np.array([kpts2[m.trainIdx].pt for m in matches], dtype=np.float32).T  # Shape: 2xN\n",
    "\n",
    "    # Perform triangulation using OpenCV\n",
    "    points_4d_homogeneous = cv2.triangulatePoints(P1, P2, pts1, pts2)\n",
    "\n",
    "    # Convert homogeneous coordinates to 3D (divide by w)\n",
    "    points_3d = points_4d_homogeneous[:3] / points_4d_homogeneous[3]\n",
    "\n",
    "    # Transpose to return points in Nx3 format\n",
    "    return points_3d.T\n",
    "\n",
    "def add_new_image(path, past_id, past_image, past_P, past_keys, past_descs):\n",
    "    # load next image\n",
    "    image_new = cv2.imread(path)\n",
    "\n",
    "    # extract keypoints\n",
    "    keys_new, descs_new = detect_and_draw_features(image_new, detector_type='SIFT')\n",
    "\n",
    "    # match to previous image\n",
    "    matches_new = match_and_draw_features(past_image, image_new, past_keys, past_descs, keys_new, descs_new, matcher_type='BF')\n",
    "\n",
    "    # compute camera position from p3p\n",
    "    pts_2d, pts_3d = find_2d_3d_correspondences(matches_new, keys_new, past_id)\n",
    "    print(\"Found \",pts_3d.shape[0],\" 2D-3D point correspondences.\")\n",
    "\n",
    "    # Compute camera pose for the new image using P3P algorithm\n",
    "    R, T, inliers = compute_pose_p3p(K, pts_3d, pts_2d)\n",
    "    camera_poses.append(np.hstack((R, T))) # add the new camera pose\n",
    "\n",
    "    # Update the projection matrix for the new image\n",
    "    P_new = compute_projection_matrix(K, R, T)\n",
    "\n",
    "    # filter matches using epipolar constraint and projection matrices\n",
    "    matches_new = filter_matches_using_epipolar_constraint(matches_new, past_keys, keys_new)\n",
    "    \n",
    "    print(\"Projection Matrix for Camera \",past_id+2,\" (P_new):\")\n",
    "    print(P_new)\n",
    "\n",
    "    # Triangulate 3D points using the new projection matrix\n",
    "    new_points_3d = triangulate_3d_points_matches(past_P, P_new, past_keys, keys_new, matches_new)\n",
    "\n",
    "    # Update the 3D structure and keypoint-to-structure map\n",
    "    update_structure_and_map(structure, keypoint_to_structure_map, past_keys, keys_new, matches_new, past_id, past_id + 1, new_points_3d)\n",
    "\n",
    "    return past_id+1, image_new, P_new, keys_new, descs_new, new_points_3d\n",
    "\n",
    "# we will define 'past' image data (image 2) to automatically add new images\n",
    "last_id = 1\n",
    "last_keys = keys2\n",
    "last_descs = descs2\n",
    "last_image = image2\n",
    "last_P = P2\n",
    "\n",
    "last_id, last_image, last_P, last_keys, last_descs, new_points_3d = add_new_image(image_paths[last_id + 1] , last_id, last_image, last_P, last_keys, last_descs)\n",
    "points_3d = np.vstack((points_3d, new_points_3d))\n",
    "plot_3d_points_interactive(points_3d, camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c4385-949a-44f9-87aa-68e7eec9cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id, last_image, last_P, last_keys, last_descs, new_points_3d = add_new_image(image_paths[last_id + 1] , last_id, last_image, last_P, last_keys, last_descs)\n",
    "points_3d = np.vstack((points_3d, new_points_3d))\n",
    "plot_3d_points_interactive(points_3d, camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b05038-79f9-4020-9bb9-58b6fd441510",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id, last_image, last_P, last_keys, last_descs, new_points_3d = add_new_image(image_paths[last_id + 1] , last_id, last_image, last_P, last_keys, last_descs)\n",
    "points_3d = np.vstack((points_3d, new_points_3d))\n",
    "plot_3d_points_interactive(points_3d, camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9564435-476b-40da-8613-b176d5bceddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id, last_image, last_P, last_keys, last_descs, new_points_3d = add_new_image(image_paths[last_id + 1] , last_id, last_image, last_P, last_keys, last_descs)\n",
    "plot_3d_points_interactive(points_3d, camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cdb905-dcfb-4b05-aace-cb2d7901890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id, last_image, last_P, last_keys, last_descs, new_points_3d = add_new_image(image_paths[last_id + 1] , last_id, last_image, last_P, last_keys, last_descs)\n",
    "points_3d = np.vstack((points_3d, new_points_3d))\n",
    "plot_3d_points_interactive(points_3d, camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c227ce8-a29e-4474-8b18-014cc0c4e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can try adding rest of the images in the dataset\n",
    "# self todo: optimization (refinement)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
